---
title: "LLM Fine-tuning: Day 1"
date: 2025-07-07
categories: [LLM Fine-tuning]
---

# Simulating/testing 

Before diving into the project, I tested out gpt-2 using the pipeline method on Google Colab.

First, I installed all required libraries to run the model
```bash
pip install transformers datasets accelerate -q
```

Then, I ran a quick test to check whether confirm the program is running in colabs and the GPU (I selected T4 GPU for convenience)
<img width="1007" alt="Screenshot 2025-07-07 at 3 16 24 PM" src="https://github.com/user-attachments/assets/b913cde6-f725-4572-99a2-d957f9fd04af" />

After that, I implemented a simple program to test gpt-2's text generation, summarization, etc capabilities. Below is a snippet of the text generation code:

<img width="721" alt="Screenshot 2025-07-07 at 3 19 49 PM" src="https://github.com/user-attachments/assets/682986e6-2393-443b-a759-141f753e55e4" />

The model was able to properly generate texts according to the 3 given prompts

<img width="1200" alt="Screenshot 2025-07-07 at 3 21 42 PM" src="https://github.com/user-attachments/assets/189234c1-423a-4dcf-91e6-90898ed7ce6c" />
<img width="1200" alt="Screenshot 2025-07-07 at 3 21 53 PM" src="https://github.com/user-attachments/assets/5a649fa7-6a08-441e-bf79-569b40619c2e" />
<img width="1200" alt="Screenshot 2025-07-07 at 3 22 09 PM" src="https://github.com/user-attachments/assets/ee66c080-611d-46ed-a146-e2963f8deb95" />

*The model also produced confident responses for sentimental analysis, Q&A, and summarization.*

Now that I have a good idea of how llms work, I will begin working on my project.
