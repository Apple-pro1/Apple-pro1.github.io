---
title: "LLM Fine-tuning: Day 3"
date: 2025-07-10
categories: [LLM Fine-tuning]
---

# Data Cleaning and Tokenization

Today, I worked on coding scripts to test the data quality, clean the data, tokenize the data for GPT-2, and finally test the tokenized data.

**Step 1: Testing Data Quality**

I start by checking the data's basic structure in order to eliminate any basic low-quality data (e.g. empty data)

<img width="559" height="702" alt="Screenshot 2025-07-10 at 3 44 51 PM" src="https://github.com/user-attachments/assets/4d8f08e8-131c-41c3-8627-a9908e90cd50" />

After that, I check the quality of the code by verifying whether the code is sytactically valid, has functions, has classes, has imports, and whether they are too long or too short. I define too short as < 2 and too long as > 50 for code length.

<img width="581" height="825" alt="Screenshot 2025-07-10 at 3 45 16 PM" src="https://github.com/user-attachments/assets/3b2478f2-863a-4787-ad72-053bd0ee6aea" />

Then, I check the quality of each code's explaination by parsing for common terms.

<img width="698" height="718" alt="Screenshot 2025-07-10 at 3 47 17 PM" src="https://github.com/user-attachments/assets/adeff1b9-a35c-4dc6-979c-d4ca0e8ff4c4" />

Finally, I generate a report for each split so that it's easy to access on my repo.

<img width="328" height="239" alt="Screenshot 2025-07-10 at 3 48 29 PM" src="https://github.com/user-attachments/assets/c9aa1a95-58d5-4de2-b330-155ead81eef2" />

**Step 2: Cleaning Data**

After testing the data quality, I clean the data by first removing invalid examples and cleaning/formatting the code & explainations.

<img width="447" height="478" alt="Screenshot 2025-07-10 at 3 52 11 PM" src="https://github.com/user-attachments/assets/4b040a96-f6f8-46b6-a9ab-3244e47e09c2" />
<img width="444" height="717" alt="Screenshot 2025-07-10 at 3 52 32 PM" src="https://github.com/user-attachments/assets/52481075-fb48-44e3-8f6b-ceafba7ee861" />

**Step 3: Tokenizing Data**

With all the data clean, I prepare them for tokenization for the GPT-2 model. I tokenize everydata set and make them compatible for Hugging Face and GPT-2

<img width="520" height="696" alt="Screenshot 2025-07-10 at 3 53 29 PM" src="https://github.com/user-attachments/assets/f83f48bb-0770-4f78-87f6-ae342a59775d" />

I also statistically analyze the token lengths for understand the size of the data I'm dealing with.

<img width="518" height="404" alt="Screenshot 2025-07-10 at 3 54 27 PM" src="https://github.com/user-attachments/assets/dbb9a97a-c567-463b-8c88-af4ee5799b5f" />

**Repeat**

These three steps are repeated for each data split (train, test, validation)

Example, step 1 for the test split:

<img width="235" height="148" alt="Screenshot 2025-07-10 at 3 57 31 PM" src="https://github.com/user-attachments/assets/cab991f3-d8f5-4c0c-863b-d7e59e4d58cd" />

Example, step 2 for the test split:

<img width="280" height="41" alt="Screenshot 2025-07-10 at 3 58 03 PM" src="https://github.com/user-attachments/assets/af6f7026-e73e-4357-a261-1d0fe1a8a8a5" />

Example, step 3 for the test split:

<img width="937" height="211" alt="Screenshot 2025-07-10 at 3 58 15 PM" src="https://github.com/user-attachments/assets/82487339-668a-419d-8929-6f2f33bef363" />


**Testing Tokenized Data Quality**

After tokenizing all the data, I created another test to verify the quality of the tokenized data.

<img width="480" height="410" alt="Screenshot 2025-07-10 at 3 59 46 PM" src="https://github.com/user-attachments/assets/807912f2-1b26-462f-8ef6-ca6aff757102" />

I also check for data consistency across the different data splits.

<img width="560" height="542" alt="Screenshot 2025-07-10 at 4 00 31 PM" src="https://github.com/user-attachments/assets/7a20b5a4-9a4c-468e-9d72-3240edb07b4e" />

Additionally, I created a visial representation of the data quality of all splits.

<img width="1127" height="745" alt="Screenshot 2025-07-10 at 4 01 00 PM" src="https://github.com/user-attachments/assets/d8a3dd74-67a0-4dd9-9cd9-04402c8ed589" />

Fortunately, most of the data was valid, meaning that I have a good chunck of data to train GPT-2
- train: 164275 examples
- validation: 20713 examples
- test: 20413 examples

**Notes**
I pulled all the raw data pushed on my github repo to keep my repo lightweight and respect data licenses. Also, people can simply access the raw data by running my load-data.py script.
